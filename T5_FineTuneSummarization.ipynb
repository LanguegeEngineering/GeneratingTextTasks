{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Working with Hugging Face Models and Datasets\n",
        "## Chapter 3: Text Summarization Using Models in Hugging Face\n",
        "### Lesson 3.2: Fine-tuning the pre-trained T5-small model in Hugging Face for text summarization\n",
        "\n",
        "In this lesson, we will fine-tune the [T5-small](https://huggingface.co/t5-small) model on the California state bill subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset. We can also fine-tune other models including Google's PEGASUS model. However, for illustration, we only demonstrate the fine-tuning steps using the smaller model, t5-small, in this tutorial."
      ],
      "metadata": {
        "id": "P-gOptb-RoTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Transformers and Datasets from Hugging Face"
      ],
      "metadata": {
        "id": "FtxOv_uTSC4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3Kf0jYlkd9c"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install -q transformers[torch] datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7HJ10uskwfZ"
      },
      "source": [
        "## Load BillSum dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJf6uIWEkwfZ"
      },
      "source": [
        "Let us load the [BillSum](https://huggingface.co/datasets/billsum) dataset from the Huggingface Datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h422bL0Pkwfa"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loaded billsum dataset only has one Dataset object:"
      ],
      "metadata": {
        "id": "WVsxnTiN3WK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum"
      ],
      "metadata": {
        "id": "4cXK4DaT3SPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WCy-sEikwfb"
      },
      "source": [
        "For fine-tuning and late evaluation, we should split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-KWJ-HLkwfb"
      },
      "outputs": [],
      "source": [
        "billsum = billsum.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that we have a train and test Dataset:"
      ],
      "metadata": {
        "id": "OdAIhcDy3vZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum"
      ],
      "metadata": {
        "id": "94QRoN_n3mOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LMxpWYnkwfb"
      },
      "source": [
        "Take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qT255xdkwfb"
      },
      "outputs": [],
      "source": [
        "example = billsum[\"train\"][0]\n",
        "for key in example:\n",
        "    print(\"A key of the example: \\\"{}\\\"\".format(key))\n",
        "    print(\"The value corresponding to the key-\\\"{}\\\"\\n \\\"{}\\\"\".format(key, example[key]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKlg8d6hkwfc"
      },
      "source": [
        "There are three fields:\n",
        "\n",
        "- `text`: the text of the bill.\n",
        "- `summary`: a given summary of the text.\n",
        "- `title`: the title of the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi8va4b5kwfc"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqzh7-EJkwfc"
      },
      "source": [
        "We will fine-tune the T5-small model. At the Overview page of the [Hugging Face T5 model](https://huggingface.co/docs/transformers/model_doc/t5#overview), it provides the following tips:\n",
        "- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.\n",
        "- T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: ….\n",
        "\n",
        "We will load a T5 tokenizer to process `text` and `summary` and prepend a prefix \"summarize: \" for our text summarization task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvlavpV8kwfc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the tokenizer on an example:"
      ],
      "metadata": {
        "id": "m2gSmJO35V7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer(example['text'])\n",
        "for key in tokenized_text:\n",
        "    print(key)\n",
        "    print(tokenized_text[key])"
      ],
      "metadata": {
        "id": "3EVe1Vi85ZMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPmIYLsBkwfd"
      },
      "source": [
        "We will create a function to preprocess the training and test data in batch. The preprocessing function will perform the following actions:\n",
        "- Prepend the prefix \"summarize: \" to each text document to indicate to the T5 model that the task at hand is summarization.\n",
        "- Convert the input texts and summary labels into a tokenized format that can be processed by the T5 model.\n",
        "- Set the max_length parameter to ensure that the tokenized inputs and labels do not exceed a certain length, truncating any text that is too long.\n",
        "- Assign the tokenized labels to the labels field of model_inputs, which will be used during training to calculate the loss and optimize the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcU6eEvUkwfd"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    # Prepends the string \"summarize: \" to each document in the 'text' field of the input examples.\n",
        "    # This is done to instruct the T5 model on the task it needs to perform, which in this case is summarization.\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"text\"]]\n",
        "\n",
        "    # Tokenizes the prepended input texts to convert them into a format that can be fed into the T5 model.\n",
        "    # Sets a maximum token length of 1024, and truncates any text longer than this limit.\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    # Tokenizes the 'summary' field of the input examples to prepare the target labels for the summarization task.\n",
        "    # Sets a maximum token length of 128, and truncates any text longer than this limit.\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    # Assigns the tokenized labels to the 'labels' field of model_inputs.\n",
        "    # The 'labels' field is used during training to calculate the loss and guide model learning.\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Returns the prepared inputs and labels as a single dictionary, ready for training.\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ME6dZkkwfd"
      },
      "source": [
        "Let us apply the preprocessing function over the entire dataset, use Huggingface Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. We can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtLAFzwWkwfe"
      },
      "outputs": [],
      "source": [
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us take a look at a test example:"
      ],
      "metadata": {
        "id": "UrNKVnTh8ncB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_billsum['test'][0]['text']"
      ],
      "metadata": {
        "id": "AFl6InMXjdVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_billsum['test'][0]['summary']"
      ],
      "metadata": {
        "id": "dJ15m-Ho8yuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp2WIq-nkwfe"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NpcPjMqkwff"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapKiy3Ykwff"
      },
      "source": [
        "## Evaluation Metrics for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO35IUHIkwff"
      },
      "source": [
        "We will use the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric for training. We will load the evaluation method from the Huggingface [Evaluate](https://huggingface.co/docs/evaluate/index) library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q evaluate rouge_score"
      ],
      "metadata": {
        "id": "V715N50V9l4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1C0Bzf6kwff"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flqokcnukwff"
      },
      "source": [
        "Create a function that passes the predictions and labels to calculate the ROUGE metric as follows:\n",
        "- The eval_pred tuple is unpacked into predictions and labels.\n",
        "- The tokenizer.batch_decode method is used to decode the tokenized predictions and labels back to text, skipping any special tokens like padding tokens.\n",
        "- The np.where function is used to replace any instances of -100 in the labels array with the tokenizer's pad_token_id, as -100 is often used to signify tokens that should be ignored during loss calculation.\n",
        "- The rouge.compute method is called to calculate the ROUGE metric between the predictions and labels, which is a common metric for evaluating text summarization performance.\n",
        "- The length of each prediction is calculated by counting the number of non-padding tokens, and the mean prediction length is added to the result dictionary under the key \"gen_len\".\n",
        "- Finally, the values in the result dictionary are rounded to 4 decimal places for cleaner output, and the result is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g15bNmxokwff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Unpacks the evaluation predictions tuple into predictions and labels.\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decodes the tokenized predictions back to text, skipping any special tokens (e.g., padding tokens).\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replaces any -100 values in labels with the tokenizer's pad_token_id.\n",
        "    # This is done because -100 is often used to ignore certain tokens when calculating the loss during training.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decodes the tokenized labels back to text, skipping any special tokens (e.g., padding tokens).\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Computes the ROUGE metric between the decoded predictions and decoded labels.\n",
        "    # The use_stemmer parameter enables stemming, which reduces words to their root form before comparison.\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Calculates the length of each prediction by counting the non-padding tokens.\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "\n",
        "    # Computes the mean length of the predictions and adds it to the result dictionary under the key \"gen_len\".\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    # Rounds each value in the result dictionary to 4 decimal places for cleaner output, and returns the result.\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_BMk4H_kwfg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer classes from the Hugging Face transformers library:"
      ],
      "metadata": {
        "id": "rEmKH-7-A0dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZwDj2ckkwfg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the t5-small model:"
      ],
      "metadata": {
        "id": "s8TNoxweBEod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "s8Ppf4jXBIKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training hyperparameters in Seq2SeqTrainingArguments. Assign a value to the parameter `output_dir` to specify the location to save the model. It is a required parameter."
      ],
      "metadata": {
        "id": "h_erYlheBXn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_fine_tuned_t5_small_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "V94ZAH0dBeF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the training arguments to Seq2SeqTrainer along with the model, dataset, tokenizer, data collator, and the `compute_metrics` function."
      ],
      "metadata": {
        "id": "B_mLh7reB7R_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUj4uf58kwfg"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_billsum[\"train\"],\n",
        "    eval_dataset=tokenized_billsum[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call train() to fine tune the model:"
      ],
      "metadata": {
        "id": "CQoqu6IsCMG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "8742zfs0CPeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations: The function `compute_metrics` worked during the training. At the last epoch, we have rouge1 value 0.1397, rouge2 value 0.1168, and rougelsum 0.1168."
      ],
      "metadata": {
        "id": "81o0htp6HGnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model:"
      ],
      "metadata": {
        "id": "_fePMzAUEpHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"my_fine_tuned_t5_small_model\")"
      ],
      "metadata": {
        "id": "N1YS89b7ErXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKkkwTXHkwfh"
      },
      "source": [
        "## Use the Fine-Tuned Model to Summarize Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nal7b6Hjkwfh"
      },
      "source": [
        "We have fine-tuned the t5-small model on the billsum dataset. We can use it for inference.\n",
        "\n",
        "We will use an example from the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SdkCW1okwfh"
      },
      "outputs": [],
      "source": [
        "text = billsum['test'][100]['text']\n",
        "text = \"summarize: \" + text\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QckhZ2u6kwfm"
      },
      "source": [
        "The simplest way to try out your fine-tuned model for inference is to use it in a pipeline(). Create a `pipeline` object for summarization with the fine-tuned model, and pass the text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k00PHZO-kwfm"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"my_fine_tuned_t5_small_model\")\n",
        "pred = summarizer(text)\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9U3Lr2dkwfn"
      },
      "source": [
        "We can also manually replicate the results of the `pipeline`.\n",
        "\n",
        "\n",
        "Tokenize the text and return the `input_ids` as PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEU3mYPrkwfn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"my_fine_tuned_t5_small_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj1QMl24kwfn"
      },
      "source": [
        "Use the generate() method to create the summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlluAdX1kwfn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"my_fine_tuned_t5_small_model\")\n",
        "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdORfhybkwfn"
      },
      "source": [
        "Decode the generated token ids back into text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9fi3Wf4kwfn"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the result\n",
        "We can compute the rouge values for the predicted summary comparing to the given summary."
      ],
      "metadata": {
        "id": "fxed9hGKFKSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred[0]['summary_text']"
      ],
      "metadata": {
        "id": "QDC7cN33FdEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = [pred[0]['summary_text']]"
      ],
      "metadata": {
        "id": "eSEykJVbFQYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [billsum['test'][100]['summary']]"
      ],
      "metadata": {
        "id": "d3XpygXUFlmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge.compute(predictions=preds, references=labels, use_stemmer=True)"
      ],
      "metadata": {
        "id": "-Fq50sIlFMb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!! We have fine-tuned a pre-trained model in Hugging Face for text summarization."
      ],
      "metadata": {
        "id": "xXVtJO6lLQvt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}