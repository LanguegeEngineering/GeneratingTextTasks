{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune DistilGPT2 and Generate Text",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C9zObeOoEoN"
      },
      "source": [
        "\n",
        "Here is a tutorial about generating text using a SOTA inspired language generation model, distilgpt2. This model lighter in weight and faster in language generation than the original OpenAI GPT2. Using this tutorial, you can train a language generation model which can generate text for any subject in English. Here, we will generate movie reviews by fine-tuning distilgpt2 on a sample of IMDB movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8OfkKEhPPu"
      },
      "source": [
        "Click on the link below and a file will be downloaded containing IMDB sample dataset of 1000 samples\n",
        "\n",
        "http://files.fast.ai/data/examples/imdb_sample.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bteQwqhsUf"
      },
      "source": [
        "Upload this file in this colab notebook using the upload button on the top left "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCqeIu66WrOJ"
      },
      "source": [
        "### Extract the csv file from the uploaded tgz file\n",
        "\n",
        "import tarfile\n",
        "with tarfile.open('imdb_sample.tgz', 'r:gz') as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziTUQXO7Yxve"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9GZEhpZRtz"
      },
      "source": [
        "data = pd.read_csv('imdb_sample/texts.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSU8OLAhZada"
      },
      "source": [
        "### This is how the CSV look like\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y55Nj02it0c"
      },
      "source": [
        "Let's get the number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-85131-nifk5"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRvpRcHai0qa"
      },
      "source": [
        "For Finetuning distilgpt2, we just need the text field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAbMLfxrZbKG"
      },
      "source": [
        "texts = list(set(data['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buC-SCx-Zkty"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUXGPS5Li-E4"
      },
      "source": [
        "Store the reviews in a txt file where each line of txt file is a single review "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotaG9qgZmHy"
      },
      "source": [
        "file_name = 'testing.txt'\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(\" |EndOfText|\\n\".join(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwFvrFRjOwo"
      },
      "source": [
        "Now, let's come to Transformers by Huggingface, and unleash the Transformers (Autobots... just kidding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkocIBHfaZul"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIJfTnDjmG6"
      },
      "source": [
        "Make 2 directories. \n",
        "\n",
        "1) weights - for storing the weights of distilgpt2\n",
        "\n",
        "2) tokenizer - for storing the tokenizer of distilgpt2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhH0xD1-qnh-"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HiZgs-kFLg"
      },
      "source": [
        "Now, its time for Training (or fine tuning) distilgpt2 with IMDB reviews\n",
        "Given below is a command containing few parameters to help Transformers finetune distilgpt2. now, let's understand what these parameters mean\n",
        "\n",
        "1) output_dir: It is the weights_dir we made where our finetuned model will be stored in the form of checkpoints\n",
        "\n",
        "2) model_name_or_path: It tells the kind of model we are currently dealing with\n",
        "\n",
        "3) per_device_train_batch_size: It tells the batch size for each gpu\n",
        "\n",
        "4) do_train: It tells pytorch to start training mode\n",
        "\n",
        "5) train_file: This is where we give the input text data \n",
        "\n",
        "6) num_train_epochs: Number of epochs for finetuning\n",
        "\n",
        "\n",
        "Now, let the training begin..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_rFZRe_2KAW"
      },
      "source": [
        "weights_dir = \"output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42gszj3gkUU1"
      },
      "source": [
        "cmd = '''\n",
        "python transformers/examples/tensorflow/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file {0} \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --output_dir {1}\n",
        "'''.format(file_name, weights_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQKT9jlOcnjY"
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl2zr2Gm2Db"
      },
      "source": [
        "Although, Huggingface provides a run_generation.py file for language generation. Running it from a command (as it takes the input), makes it load the model and the tokenizer everytime you run the file which slows downs generation. To reduce the I/O overhead, I have restructured the run_generation.py file in the following code which only loads the model and tokenizer once in a model and a tokenizer object and we can use these objects to generate text over and over again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_-L9gGziiv"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def get_model_tokenizer(weights_dir, device = 'cuda'):\n",
        "    print(\"Loading Model ...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(weights_dir, from_tf=True)\n",
        "    model.to('cuda')\n",
        "    print(\"Model Loaded ...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\", from_tf=True)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = 0.7,\n",
        "    k=20,\n",
        "    p=0.9,\n",
        "    repetition_penalty = 1.0,\n",
        "    device = 'cuda'\n",
        "):\n",
        "\n",
        "    MAX_LENGTH = int(10000)\n",
        "    def adjust_length_to_model(length, max_sequence_length):\n",
        "        if length < 0 and max_sequence_length > 0:\n",
        "            length = max_sequence_length\n",
        "        elif 0 < max_sequence_length < length:\n",
        "            length = max_sequence_length  # No generation bigger than model size\n",
        "        elif length < 0:\n",
        "            length = MAX_LENGTH  # avoid infinite loop\n",
        "        return length\n",
        "        \n",
        "    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return generated_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3fwyPATznLp"
      },
      "source": [
        "\n",
        "model, tokenizer = get_model_tokenizer(weights_dir, device = 'cuda')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqpCwzTUz0wZ"
      },
      "source": [
        "temperature = 1.0\n",
        "k=400\n",
        "p=0.9\n",
        "repetition_penalty = 1.0\n",
        "num_return_sequences = 5\n",
        "length = 1000\n",
        "stop_token = '|EndOfText|'\n",
        "prompt_text = \"this is\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQv52LcT0Iyi"
      },
      "source": [
        "%%time\n",
        "generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = temperature,\n",
        "    k=k,\n",
        "    p=p,\n",
        "    repetition_penalty = repetition_penalty\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXtDNZ4dunQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}