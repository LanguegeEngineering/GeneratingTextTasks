{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2FII6Mi9_Xq"
      },
      "source": [
        "# Introducing BART\n",
        "> Episode 1 -- a mysterious new Seq2Seq model with state of the art summarization performance visits a popular open source library\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [jupyter]\n",
        "- image: images/text_infilling.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Sh-vhX9_X0"
      },
      "source": [
        "### Overview\n",
        "\n",
        "For the past few weeks, I worked on integrating BART into [transformers](https://github.com/huggingface/transformers/). This post covers the high-level differences between BART and its predecessors and how to use the new `BartForConditionalGeneration` to summarize documents. Leave a comment below if you have any questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWYl1egU9_X1"
      },
      "source": [
        "### Background: Seq2Seq Pretraining\n",
        "In October 2019, teams from Google and Facebook published new transformer papers:  [T5](https://arxiv.org/abs/1910.10683) and [BART](https://arxiv.org/abs/1910.13461). Both papers achieved better downstream performance on generation tasks, like abstractive summarization and dialogue, with two changes:\n",
        "- add a causal decoder to BERT's bidirectional encoder architecture\n",
        "- replace BERT's fill-in-the blank cloze task with a more complicated mix of pretraining tasks.\n",
        "\n",
        "<!-- **Tasks:** Historically, Seq2Seq models have been used for text generation tasks like summarization and translation. \"BART is particularly effective when finetuned for text generation, but also matches the performance of RoBERTa on GLUE and SQuAD\", with only 10% more parameters. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Le1XigS9_YC"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnRvK1gb9_YD"
      },
      "source": [
        "In summarization tasks, the `input` sequence is the document we want to summarize, and the `output` sequence is a ground truth summary.\n",
        "Seq2Seq archictectures can be directly finetuned on summarization tasks, without any new randomly initialized heads. The pretraining task is also a good match for the downstream task. In both settings, the input document must be copied from the input with modification. The numbers confirm this: all the new fancy Seq2Seq models do a lot better than the old less-fancy guys on the CNN/Daily Mail abstractive summarization task, and BART does especially well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_QIkq9w9_YE"
      },
      "source": [
        "|                Model |   Rouge2 | Model Size   | Pretraining   |\n",
        "|:---------------------|---------:|:-------------|:--------------|\n",
        "| PT-Gen               |    17.28 | 22 M         | None          |\n",
        "| TransformerAbs       |    17.76 | 200M         | None          |\n",
        "| BertSumABS           |    19.39 | 220 M        | Encoder       |\n",
        "| UniLM                |    20.3  | 340 M        | Seq2Seq       |\n",
        "| T5-base              |    20.34 | 770 M        | Seq2Seq       |\n",
        "| Bart                 |    21.28 | 406 M        | Seq2Seq       |\n",
        "| T5-11B               |    21.55 | 11 B         | Seq2Seq       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjY1wzk89_YE"
      },
      "source": [
        "- `BertSumABS` (from [*Text Summarization with Pretrained Encoders*](https://arxiv.org/abs/1908.08345), uses a Seq2Seq architecture but doesn't pretrain the decoder. `TransformerAbs`, from the same paper, uses a slightly smaller model and no pretraining. \n",
        "- `PT-Gen` is from [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368.pdf)\n",
        "- [UniLM](https://arxiv.org/abs/1905.03197) is a \"Prefix-LM\" with a similar masking strategy to Bart and T5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R2NOFlo9_YF"
      },
      "source": [
        "### Demo: BartForConditionalGeneration "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynlz6D5dEVuk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1_i9bls9_YG"
      },
      "outputs": [],
      "source": [
        "#collapse-hide\n",
        "INSTALL_MSG = \"\"\"\n",
        "Bart will be released through pip in v 3.0.0, until then use it by installing from source:\n",
        "\n",
        "git clone git@github.com:huggingface/transformers.git\n",
        "git checkout d6de6423\n",
        "cd transformers\n",
        "pip install -e \".[dev]\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "try:\n",
        "    import transformers\n",
        "    from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "except ImportError:\n",
        "    raise ImportError(INSTALL_MSG)\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "LONG_BORING_TENNIS_ARTICLE = \"\"\"\n",
        " Andy Murray  came close to giving himself some extra preparation time for his w\n",
        "edding next week before ensuring that he still has unfinished tennis business to\n",
        " attend to. The world No 4 is into the semi-finals of the Miami Open, but not be\n",
        "fore getting a scare from 21 year-old Austrian Dominic Thiem, who pushed him to \n",
        "4-4 in the second set before going down 3-6 6-4, 6-1 in an hour and three quarte\n",
        "rs. Murray was awaiting the winner from the last eight match between Tomas Berdy\n",
        "ch and Argentina's Juan Monaco. Prior to this tournament Thiem lost in the secon\n",
        "d round of a Challenger event to soon-to-be new Brit Aljaz Bedene. Andy Murray p\n",
        "umps his first after defeating Dominic Thiem to reach the Miami Open semi finals\n",
        " . Muray throws his sweatband into the crowd after completing a 3-6, 6-4, 6-1 vi\n",
        "ctory in Florida . Murray shakes hands with Thiem who he described as a 'strong \n",
        "guy' after the game . And Murray has a fairly simple message for any of his fell\n",
        "ow British tennis players who might be agitated about his imminent arrival into \n",
        "the home ranks: don't complain. Instead the British No 1 believes his colleagues\n",
        " should use the assimilation of the world number 83, originally from Slovenia, a\n",
        "s motivation to better themselves. At present any grumbles are happening in priv\n",
        "ate, and Bedene's present ineligibility for the Davis Cup team has made it less \n",
        "of an issue, although that could change if his appeal to play is allowed by the \n",
        "International Tennis Federation. Murray thinks anyone questioning the move, now \n",
        "it has become official, would be better working on getting their ranking closer \n",
        "to his. 'If he was 500 in the world they wouldn't be that fussed about it but ob\n",
        "viously he threatens their position a bit,' said the 27 year-old Scot. ' and he'\n",
        "s obviously the British number two, comfortably. 'So they can complain but the b\n",
        "est thing to do is use it in the right way and accept it for what it is, and try\n",
        " to use it as motivation whether they agree with it or not. He's British now so \n",
        "they've just got to deal with it. Murray stretches for a return after starting h\n",
        "is quarter final match slowly on the show court . Thiem held nothing back as he \n",
        "raced through the opening set, winning it 6-3 with a single break . The young Au\n",
        "strian is considered to be one of the hottest prospects on the ATP Tour . 'I wou\n",
        "ld hope that all the guys who are below him now like James (Ward) , Kyle (Edmund\n",
        ") , Liam (Broady) they will use it as motivation. If he becomes eligible for Dav\n",
        "is Cup then those guys are going to have to prove themselves. 'It can only be se\n",
        "en as a positive for those guys using it to try to get better. He's a good playe\n",
        "r but so are James and Kyle and Liam has improved. Aljaz is there, he's on the t\n",
        "our every week, the other guys aren't quite there yet.' For the first time Murra\n",
        "y, who has an encyclopaedic knowledge of the top 100, gave his opinion of Bedene\n",
        ": 'He's a good player with a very good serve. He's a legitimate top 100 player, \n",
        "when he plays Challengers he's there or thereabouts, when he plays on the main t\n",
        "our he wins matches, it's not like he turns up and always loses in the first rou\n",
        "nd. Murray's fiancee was once again watching from the stands shaded by a huge br\n",
        "immed hat . Kim Sears flashes her enormous diamond engagement ring while watchin\n",
        "g her beau on court . 'He had a bad injury last year (wrist) but has recovered w\n",
        "ell. I would imagine he would keep moving up the rankings although I don't know \n",
        "exactly how high he can go. I've practised with him a couple of times, I haven't\n",
        " seen him play loads, but when you serve as well as he does it helps. I would im\n",
        "agine he' s going to be comfortably in the top 70 or 80 in the world for a while\n",
        ".' It is understood the Lawn Tennis Association will give background support to \n",
        "his case regarding the Davis Cup but have made it clear that the onus is on him \n",
        "to lead the way. An official statement said: 'To have another player in the men'\n",
        "s top 100 is clearly a positive thing for British tennis and so we very much wel\n",
        "come Aljaz's change in citizenship.' The last comparable switch came twenty year\n",
        "s ago when Greg Rusedski arrived from Canada. It was by no means universally pop\n",
        "ular but, like Bedene, he pledged that he was in for the long haul and, in fairn\n",
        "ess to him, he proved true to his word. Loising the first set shocked Murray int\n",
        "o life as he raced to a commanding lead in the second . The No 3 seed sent over \n",
        "a few glaring looks towards his team before winning the second set . Murray had \n",
        "to put such matters aside as he tackled the unusually talented Thiem, a delight \n",
        "to watch. Coached by Boris Becker's veteran mentor Gunter Bresnik, he slightly r\n",
        "esembles Andy Roddick and hits with similar power but more elegance. His single \n",
        "handed backhand is a thing of rare beauty. However, he has had a mediocre season\n",
        " coming into this event and there was little to forewarn of his glorious shotmak\n",
        "ing that seemed to catch Murray unawares early on. The world No 4 looked to have\n",
        " worked him out in the second, but then suffered one of his periopdic mental lap\n",
        "ses and let him back in from 4-1 before closing it out with a break. After break\n",
        "ing him for 3-1 in the decider the Austrian whirlwind burnt itself out. 'He's a \n",
        "strong guy who hits the ball hard and it became a very physical match,' said Mur\n",
        "ray. Murray was presented with a celebratory cake after winning his 500th match \n",
        "in the previous round .\n",
        "\"\"\".replace('\\n','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B92NKaw9_YJ"
      },
      "outputs": [],
      "source": [
        "#collapse-show\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "article_input_ids = tokenizer.batch_encode_plus([LONG_BORING_TENNIS_ARTICLE], return_tensors='pt', max_length=1024)['input_ids']\n",
        "summary_ids = model.generate(article_input_ids,\n",
        "                             num_beams=4,\n",
        "                             length_penalty=2.0,\n",
        "                             max_length=142,\n",
        "                             min_length=56,\n",
        "                             no_repeat_ngram_size=3)\n",
        "\n",
        "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "display(Markdown('> **Summary: **'+summary_txt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25fQE7qb9_YM"
      },
      "source": [
        "GPT2, which in fairness is not finetuned for summarization, cannot really continue the tennis article sensically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QJJho5N9_YN"
      },
      "outputs": [],
      "source": [
        "#collapse-show\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "gpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# truncate to 869 tokens so that we have space to generate another 155\n",
        "enc = gpt2_tok.encode(LONG_BORING_TENNIS_ARTICLE, max_length=1024-155, return_tensors='pt') \n",
        "# Generate another 155 tokens\n",
        "source_and_summary_ids = gpt2_model.generate(enc, max_length=1024, do_sample=False)\n",
        "# Only show the new ones\n",
        "end_of_source = \"An official statement said:\" \n",
        "_, summary_gpt2 = gpt2_tok.decode(source_and_summary_ids[0]).split(end_of_source)\n",
        "display(Markdown('> **GPT2:** ' + summary_gpt2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPNf-6C39_YO"
      },
      "source": [
        "More importantly, these snippets show that even though `BartForConditionalGeneration` is a Seq2Seq model, while `GPT2LMHeadModel` is not, they can be invoked in similar ways for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrBvhfW9_YP"
      },
      "source": [
        "## Conclusion\n",
        "Our first release of `BartModel` prioritized moving quickly and keeping the code simple, but it's still a work in progress. I am currently working on making the implementation in transformers faster and more memory efficient, so stay tuned for episode 2!\n",
        "\n",
        "A big thank you to Sasha Rush, Patrick von Platen, Thomas Wolf, Clement Delangue, Victor Sanh, Yacine Jernite, Harrison Chase and Colin Raffel for their feedback on earlier versions of this post, and to the BART authors for releasing their code and answering questions on GitHub."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ComparisonGPT2BART.ipynb",
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}